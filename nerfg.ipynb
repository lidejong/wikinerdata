{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nerfg.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cN9ktg4uFopo",
        "colab_type": "code",
        "outputId": "200e2164-c3b6-4fb5-8e37-266b856f8f34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import re\n",
        "import nltk\n",
        "from nltk import tokenize\n",
        "nltk.download('punkt') # Download before running the code.  \n",
        "\n",
        "S = requests.Session()\n",
        "URL = \"https://nl.wikipedia.org/w/api.php\"\n",
        "\n",
        "f = open(\"firstsections.txt\", \"w\")\n",
        "g = open(\"sws.txt\", \"w\")\n",
        "\n",
        "# Gives a list of all articles inside a Wikipedia category. \n",
        "def getCategoryList(category):\n",
        "  listArticlesCategory = []\n",
        "  PARAMS = {\n",
        "      'action': \"query\",\n",
        "      'list': \"categorymembers\",\n",
        "      'cmtitle': \"Category:\" + category,\n",
        "      'cmlimit': 440, # The limit of categories.\n",
        "      'cmprop':'title|sortkey',\n",
        "      # 'cmstarthexsortkey': '', #If you fill in the sortkey, the code will begin running at that sortkey. \n",
        "      'format': \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  for element in data['query']['categorymembers']:\n",
        "    # We do not want Wikipedia categories, only the articles inside a category.\n",
        "    if not element['title'].startswith('Categorie:'): \n",
        "      listArticlesCategory.append(element['title'])\n",
        "  \n",
        "  return listArticlesCategory\n",
        "\n",
        "# Gives a list of the parent categories from a specific Wikipedia article. \n",
        "def getParentCat(subject):\n",
        "  parentCat = []\n",
        "  PARAMS = {\n",
        "      \"action\":\"query\",\n",
        "      \"format\":\"json\", \n",
        "      \"titles\":subject,\n",
        "      \"prop\":\"categories\",\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  for value in data[\"query\"][\"pages\"].values():\n",
        "    if 'categories' in value: # Not all Wikipedia articles have a parent category.\n",
        "      for dictionary in value['categories']:\n",
        "        if not dictionary['title'].startswith('Categorie:Wikipedia:'): \n",
        "          # Wikipedia also knows hidden categories, we do not want those.\n",
        "          cat = dictionary['title']\n",
        "          parentCat.append(cat)         \n",
        "      return parentCat\n",
        "    else:\n",
        "      return subject + \" does not have any parent categories.\"\n",
        "\n",
        "# Given a certain Wikipedia article, will return a list of \n",
        "# sentences containing the title of that Wikipedia article.\n",
        "def getSentencesWithSubject(subject):\n",
        "  allSections = \"\"\n",
        "  numberSections = ['0']\n",
        "\n",
        "  PARAMS = {\n",
        "      'action': 'parse',\n",
        "      'page': subject,\n",
        "      'prop': 'sections',\n",
        "      'format': 'json'\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "\n",
        "  # Retrieves all sections.\n",
        "  if not data['parse']['sections'] == []:\n",
        "    for sectionnumber in data['parse']['sections']:\n",
        "      if sectionnumber['index'] != '':\n",
        "        numberSections.append(sectionnumber['index'])\n",
        "        for indexnumber in numberSections:\n",
        "          PARAMS = {\n",
        "          'action': \"parse\",\n",
        "          'prop': \"wikitext\",\n",
        "          'section': indexnumber,\n",
        "          'page': subject,\n",
        "          'format': \"json\"\n",
        "          }\n",
        "\n",
        "          R = S.get(url=URL, params=PARAMS)\n",
        "          data = R.json()\n",
        "\n",
        "          nextSection = data['parse']['wikitext']['*']\n",
        "          nextSection = cleanSection(nextSection, subject)\n",
        "           \n",
        "          allSections += nextSection + \" \"\n",
        "  else:\n",
        "    PARAMS = {\n",
        "    'action': \"parse\",\n",
        "    'prop': \"wikitext\",\n",
        "    'section': 0,\n",
        "    'page': subject,\n",
        "    'format': \"json\"\n",
        "    }\n",
        "\n",
        "    R = S.get(url=URL, params=PARAMS)\n",
        "    data = R.json()\n",
        "    allSections = data['parse']['wikitext']['*']\n",
        "    allSections = cleanSection(allSections, subject)\n",
        "      \n",
        "  # Makes a list of all sentences containing the subject,\n",
        "  # where links take the form of [[c]] or [[a|b]]. \n",
        "  # An example of [[a|b]] is [[China|Chinese]], in the text it is displayed 'Chinese',\n",
        "  # but the link will go to the Wikipedia article 'China'. \n",
        "  sentencesWithSubjectLine = []\n",
        "  for sentence in tokenize.sent_tokenize(allSections):\n",
        "    if \"[[\" + subject + \"]]\" in sentence:\n",
        "      sentencesWithSubjectLine.append(sentence)  \n",
        " \n",
        "  # Finds all NEs (that either look like [[c]] or [[a|b]])\n",
        "  # in the listsentencesWithSubjectLine. \n",
        "  # Including NEs such as \"'s-Hertogenbosch\", \"'s Gravenshage\" and \"'t Vossenhol\".\n",
        "  # The Ł-character is added, because of the occurence of a Polish city in one of the articles.\n",
        "  NEsInSentencesLine = []\n",
        "  for sentence in sentencesWithSubjectLine:\n",
        "    NEsInSentencesLine.append(re.findall(r\"\\[\\[(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", sentence))\n",
        "     \n",
        "  # Changes all [[a|b]] in [[b]].\n",
        "  for listLinks in NEsInSentencesLine:\n",
        "    for link in listLinks:\n",
        "      if link != '':\n",
        "        if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "          linkAfter = re.sub(\"\\[\\[(.*?)\\|\", \"[[\", link)\n",
        "          allSections = re.sub(re.escape(link), linkAfter, allSections)\n",
        "  \n",
        "  # Makes a list of all sentences containing the subject,\n",
        "  # where links take the form of [[a]]. \n",
        "  sentencesWithSubject = []\n",
        "  for sentence in tokenize.sent_tokenize(allSections):\n",
        "    if \"[[\" + subject + \"]]\" in sentence:\n",
        "      sentencesWithSubject.append(sentence)\n",
        "      \n",
        "  # Creates the dictionary {named entity: label}.\n",
        "  NEplusTags = {}\n",
        "  for listLinks in NEsInSentencesLine:\n",
        "    for link in listLinks:\n",
        "      if link != '':\n",
        "        if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "          link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "          linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "          linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "          if getURL(linkBefore) != \" does not have a link.\":\n",
        "            NEplusTags[linkAfter] = getNERtag(linkBefore)   \n",
        "        else:\n",
        "          link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "          if getURL(link) != \" does not have a link.\":\n",
        "            NEplusTags[link] = getNERtag(link)\n",
        "  \n",
        "  return sentencesWithSubject, NEplusTags\n",
        "\n",
        "def cleanSection(section, subject):\n",
        "  # Deletes the infobox, categories, files, links, subheaders, tables, footnotes and images, etc. \n",
        "  changes = [\n",
        "    ('\\{\\| class=\"wikitable\"\\n(\\|.*\\n)*\\|\\}', ''), \n",
        "    ('\\[\\[Categorie:(.*?)\\]\\]\\n?', ''),\n",
        "    ('\\[\\[Bestand(.*?)\\]\\](?:\\s-)?\\n', ''),\n",
        "    ('\\[\\[Bestand(.*?)\\]\\]$', ''),\n",
        "    ('\\[\\[Bestand(.*?)\\]\\](\\n)?', ''),\n",
        "    ('\\[\\[Image:(.*?)\\]\\]\\n', ''),\n",
        "    ('\\[\\[Image:(.*?)\\]\\]', ''),\n",
        "    ('\\[\\[Afbeelding(.*?)\\]\\]\\n', ''),\n",
        "    ('\\[\\[File(.*?)\\]\\]\\n', ''),\n",
        "    ('<ref>\\[.*?\\n?.*?\\].*?<\\/ref>', ''),\n",
        "    ('<ref name=\"\\w*\"/>', ''),    \n",
        "    ('<\\/?(.*?)>(.*?)<\\/?(.*?)>', ''),\n",
        "    ('\\{\\{.*?\\}\\}', ''),\n",
        "    ('\\{\\{(.*?)\\n\\|.*\\n(?:\\s*\\n)?(\\|.*\\n)*\\|?\\}\\}\\n*', ''),\n",
        "    ('==(.*?)==\\s', ''), \n",
        "    ('\\[http.*?\\]\\n?', ''),\n",
        "    ('\\*\\s?\\[http.*?\\]', ''), \n",
        "    (\"\\'\\'\\'\", \"\"),\n",
        "    (\"\\'\\'\", \"\"),\n",
        "    ('\\n',' ')\n",
        "  ]\n",
        "  \n",
        "  for old, new in changes:\n",
        "    section = re.sub(old, new, section)\n",
        "\n",
        "  # Finds all [[a|b]] where b starts with a lowercase letter or integer. \n",
        "  # Deletes the a| in [[a|b]] links. \n",
        "  for link in re.findall(r\"\\[\\[([^]]+)\\|'?\\.?[a-z0-9].*?\\]\\]\", section):\n",
        "    section = re.sub(re.escape(link) + r\"\\|\", \"\", section)\n",
        "  \n",
        "  # Finds all links that start with a lowercase letter or integer, \n",
        "  # and deletes the [[]] around those links. \n",
        "  for link in re.findall(r\"\\[\\['?\\.?[a-z0-9].*?\\]\\]\", section):\n",
        "    link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "    section = re.sub(r\"\\[\\[\" + re.escape(link) + r\"\\]\\]\", re.escape(link), section)\n",
        "    \n",
        "  # Words after a bullet * always start with a capital letter,\n",
        "  # even though they do not refer to a named entity. \n",
        "  # Code below will delete the [[]] that follow after a *. \n",
        "  # This code needs improvement, \n",
        "  # because it will also delete the [[]] of a named entity after a *.\n",
        "  for link in re.findall(r\"\\*\\[\\[(.*?)\\]\\]\\.?\", section):\n",
        "    link = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "    section = re.sub(r\"\\[\\[\" + re.escape(link) + r\"\\]\\]\", re.escape(link), section)\n",
        "    \n",
        "  # Sometimes the text contains words like [[Nederland]]se or [[Bloedbank]]en. \n",
        "  # The link will go to the article that is inside the [[]], \n",
        "  # but the NE is in that case not complete.  \n",
        "  # Code below will change [[a]]b to [[ab]].\n",
        "  for link in re.findall(r\"\\[\\[(\\b[A-Z].*?)\\]\\]\", section):\n",
        "    for incomplNE in re.findall(r\"\\[\\[\" + re.escape(link) + r\"\\]\\][a-z]+\", section):\n",
        "      complNE = re.sub(\"[\\[\\[\\]\\]]\", \"\", incomplNE)\n",
        "      section = re.sub(re.escape(incomplNE), \"[[\" + complNE + \"]]\", section)\n",
        "  \n",
        "  # If the word is being linked to a non-existing page, the [[]] of the link are deleted.\n",
        "  for link in re.findall(r\"\\[\\[\\s?(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", section):\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "      linkClean = re.sub(\"[\\[\\[\\]\\]]\", \"\", linkBefore)\n",
        "      if getURL(linkClean) == \" does not have a link.\":\n",
        "        section = re.sub(re.escape(linkBefore), linkClean, section)\n",
        "    else:\n",
        "      linkClean = re.sub(\"[\\[\\[\\]\\]]\", \"\", link)\n",
        "      if getURL(linkClean) == \" does not have a link.\":\n",
        "        section = re.sub(re.escape(link), linkClean, section)\n",
        "  \n",
        "  # The title of the article is also a NE, so it is put between [[]].\n",
        "  section = re.sub(\" \" + re.escape(subject) + \" \", \" [[\" + subject + \"]] \", section)\n",
        "      \n",
        "  return section\n",
        "\n",
        "# Will retrieve the first section of a Wikipedia article.\n",
        "def getFirstSection(subject):\n",
        "  linksInSectionURL = []\n",
        "  PARAMS = {\n",
        "  'action': \"parse\",\n",
        "  'prop': \"wikitext\",\n",
        "  'section': 0,\n",
        "  'page': subject,\n",
        "  'format': \"json\"\n",
        "  }\n",
        "\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  section = data['parse']['wikitext']['*']\n",
        "\n",
        "  section = cleanSection(section, subject)\n",
        "    \n",
        "  # All NEs are put into the list linksInSection.\n",
        "  linksInSection = re.findall(r\"\\[\\[(?:'[a-z][\\s-])?[A-ZŁ].*?\\]\\]\", section)\n",
        "  \n",
        "  # All NEs are stripped from [[]] and added to the list linksInSectionURL.  \n",
        "  for link in linksInSection:\n",
        "    link = re.sub(r\"\\[\\[|\\]\\]\", \"\", link)\n",
        "    linksInSectionURL.append(link)\n",
        "  \n",
        "  # If NEs in linksInSectionURL take the form of [[a|b]], a| will be deleted, \n",
        "  # so their form will be [[b]].\n",
        "  for link in linksInSectionURL:\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "      section = re.sub(re.escape(link), linkAfter, section)\n",
        "  \n",
        "  # The dictionary {named entity: label} is created.\n",
        "  NEplusTags = {}\n",
        "  for link in linksInSectionURL:\n",
        "    if re.match(\"(.*?)\\|(.*?)\", link):\n",
        "      linkAfter = re.sub(\"^((.*?)\\|)\", \"\", link)\n",
        "      linkBefore = re.sub(\"\\|(.*?)$\", \"\", link)\n",
        "      if getURL(linkBefore) != \" does not have a link.\":\n",
        "        tag1 = getNERtag(linkBefore) \n",
        "        NEplusTags[linkAfter]= tag1\n",
        "    elif getURL(link) != \" does not have a link.\":\n",
        "      tag2 = getNERtag(link)\n",
        "      NEplusTags[link] = tag2  \n",
        "\n",
        "  return (section, NEplusTags)\n",
        "\n",
        "# Determines the category of a named entity.\n",
        "queue = []\n",
        "def getNERtag(subject):\n",
        "  result = ''\n",
        "  queue.append(subject)\n",
        "  \n",
        "  while queue and result == '':\n",
        "    cat = queue.pop(0)\n",
        "    result = findCat(cat)\n",
        "\n",
        "  return result\n",
        "      \n",
        "# Finds the parent categories of a certain Wikipedia article. \n",
        "def findCat(subject): \n",
        "  PARAMS = {\n",
        "      'action': 'query',\n",
        "      'format': 'json',\n",
        "      'pageids': getURL(subject),\n",
        "  }\n",
        "  \n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  title = data['query']['pages'][getURL(subject)]['title']\n",
        "  \n",
        "  parentCategories = getParentCat(title)\n",
        "  \n",
        "  if parentCategories == subject + \" does not have any parent categories.\":\n",
        "    return 'misc'\n",
        "  \n",
        "  sti = ['stichting']\n",
        "  org = ['organisatie', 'vereniging', 'kerkgenootschap', 'fonds', 'ministerie', \n",
        "        'partij', 'omroep', 'krijgsmacht', 'universiteit', 'bestuursorgaan', 'raad', 'instituut',\n",
        "        'museum', 'instantie', 'bedrijf']\n",
        "  per = ['persoon', 'hoogleraar', 'wetenschapper', 'schrijver', 'politicus',\n",
        "        'ontwerper', 'god ']\n",
        "  loc = ['geografie', 'plaats', 'rijk', 'continent', 'provincie', \n",
        "        'meer', 'land in', 'monument', 'bouwwerk', 'planeet']\n",
        "  eve = ['oorlog', 'themadag', 'evenement', 'natuurramp']\n",
        "  pro = ['taal', 'dialect', 'prijs', 'boek ', 'schrift', ' dier', 'object', 'lied', \n",
        "         'voertuig', 'vliegtuig', 'krant']\n",
        "  misc = ['stroming', 'volk', 'formaat', 'chemische stof', 'wet ', 'koninkrijk',\n",
        "         'website', 'onderwijsvorm']\n",
        "  \n",
        "  for cat in parentCategories:\n",
        "    for foundation in sti:\n",
        "      if foundation in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'sti'\n",
        "  \n",
        "  for cat in parentCategories:\n",
        "    for organisation in org:\n",
        "      if organisation in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'org'\n",
        "    for person in per:\n",
        "      if person in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'per'\n",
        "    for location in loc:\n",
        "      if location in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'loc'\n",
        "    for event in eve:\n",
        "      if event in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'eve'\n",
        "    for product in pro:\n",
        "      if product in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'pro'\n",
        "    for miscellaneous in misc:\n",
        "      if miscellaneous in cat.lower():\n",
        "        queue.clear()\n",
        "        return 'misc'\n",
        "\n",
        "  queue.extend(parentCategories)\n",
        "  \n",
        "  return ''\n",
        "\n",
        "# Will get the URL of a certain Wikipedia article.\n",
        "def getURL(subject):\n",
        "  PARAMS = {\n",
        "      'action': \"query\",\n",
        "      'titles': subject,\n",
        "      'format': \"json\",\n",
        "      'redirects': True\n",
        "  }\n",
        "  R = S.get(url=URL, params=PARAMS)\n",
        "  data = R.json()\n",
        "  \n",
        "  for pageid in data['query']['pages'].keys():\n",
        "    if pageid == \"-1\":\n",
        "      return \" does not have a link.\"\n",
        "    else:\n",
        "      return pageid\n",
        "\n",
        "def cleanWpL(text, dictOfNEsAndTags):  \n",
        "  changes = [\n",
        "    (r\"([?!:/;\\\"]+)\", r\" \\1\"), \n",
        "    (r\"([()’‘\\*]+)\", r\" \\1 \"),\n",
        "    (r\"([.,]\\s)\", r\" \\1 \"),\n",
        "    (r\"\\.$\", r\" . \"),\n",
        "    (r\"\\\\\", \"\"),\n",
        "    (r\"\\]\\]'\", \"]] '\"), # for 't Vossenhol\n",
        "    (\"  \", \" \")\n",
        "  ]\n",
        "  \n",
        "  for old, new in changes:\n",
        "    text = re.sub(old, new, text) \n",
        "  \n",
        "  # Deals with NEs that look like [[a]]-[[b]], [[a]]-b, a-[[b]].\n",
        "  NEsHyphen = re.findall(r\"\\[\\[.*?\\]\\]-(?:\\[\\[)*\\w*(?:\\]\\])*\", text)\n",
        "  for ne in NEsHyphen:\n",
        "    neNew = re.sub(r\"-\", \" - \", ne)\n",
        "    text = re.sub(re.escape(ne), neNew, text)\n",
        "   \n",
        "  # Deals with abbreviations.\n",
        "  abbreviations = re.findall(r\"[A-Z]\\s\\.(?:[A-Z]\\s\\.)*\\s\\w*\", text)\n",
        "  for abrv in abbreviations:\n",
        "    abrvNew = re.sub(r\"\\s\\.\", \".\", abrv)\n",
        "    text = re.sub(re.escape(abrv), abrvNew, text)\n",
        "    \n",
        "  # Deals with various non-alphanumeric characters inside of NEs.\n",
        "  allNEs = re.findall(r\"\\[\\[(?:.*?)\\]\\]\", text)\n",
        "  for ne in allNEs:\n",
        "    if re.search(\"\\(.*?\\)\", ne):\n",
        "      neNew = re.sub(\"\\s\\)\\s\", \")\", ne)\n",
        "      neNewer = re.sub(\"\\(\\s\", \"(\", neNew)\n",
        "      text = re.sub(re.escape(ne), neNewer, text)\n",
        "    if ' - ' in ne:\n",
        "      neNew = re.sub(\"[\\[\\[\\]\\]]\", \"\", ne)\n",
        "      if neNew not in dictOfNEsAndTags:\n",
        "        neNew = re.sub('\\s-\\s', '-', ne)\n",
        "        text = re.sub(re.escape(ne), neNew, text)\n",
        "    if re.search(\"\\[\\[\\[\\[.*?\\]\\]\", ne):\n",
        "      neNew = re.sub(\"[\\[\\[\\]\\]]\", \"\", ne)\n",
        "      text = re.sub(re.escape(ne), neNew, text)\n",
        "    if re.search(re.escape(ne) + \"\\]\\]\", text):\n",
        "      text = re.sub(\"[\\]\\]]\", \"\", text)\n",
        "    for n in re.findall(r\"\\s[?!:,\\\".’/;]+\", ne):\n",
        "      neNew = re.sub(r\"\\s\", \"\", n)\n",
        "      neNewer = re.sub(re.escape(n), neNew, ne)\n",
        "      text = re.sub(re.escape(ne), neNew, text)\n",
        "      \n",
        "  return text\n",
        "\n",
        "# Will turn a text into a token-per-line format.\n",
        "\n",
        "'''\n",
        "Because of the use of a dictionary {named entity: label}, \n",
        "named entities that have different labels in different contexts, \n",
        "such as 'Bart Smit' either being a PER or an ORG, \n",
        "will only have one key and one value. \n",
        "So, if the first occurence of 'Bart Smit' is annotated as ORG, \n",
        "all the other occurences in the text will also be annotated as ORG.\n",
        "Instead of a dictionary, an array would be a better option.\n",
        "'''\n",
        "def wordPerLine(text, dictOfNEsAndTags):\n",
        "  #print(text)\n",
        "  #print(dictOfNEsAndTags)\n",
        "  \n",
        "  # Cleans the text so that it is ready to be tokenized. \n",
        "  text = cleanWpL(text, dictOfNEsAndTags)\n",
        "  \n",
        "  # Splits the text in sentences.\n",
        "  listOfSentences = re.split(r'(?<= \\.) ', text)\n",
        "\n",
        "  output = []\n",
        "  search = \"\"\n",
        "  inTag = False\n",
        "  \n",
        "  # Will add the BIO-encoding.\n",
        "  for sentence in listOfSentences:\n",
        "    for w in sentence.split(\" \"):\n",
        "      outTag = False\n",
        "      rest = w\n",
        "\n",
        "      if rest[:2] == \"[[\":\n",
        "        rest = rest[2:]\n",
        "        inTag = True\n",
        "      if rest[-2:] == \"]]\":\n",
        "        rest = rest[:-2]\n",
        "        outTag = True\n",
        "\n",
        "      if inTag:\n",
        "        search += rest\n",
        "        if outTag:\n",
        "          val = dictOfNEsAndTags[search]\n",
        "          for word in search.split()[:1]:\n",
        "            output.append(word + \"\\tB-\" + val)\n",
        "          for word in search.split()[1:]:\n",
        "            output.append(word + \"\\tI-\" + val)\n",
        "          inTag = False\n",
        "          search = \"\"\n",
        "        else:\n",
        "          search += \" \"\n",
        "      \n",
        "      elif rest != \"\":\n",
        "        output.append(rest + \"\\tO\")\n",
        "    \n",
        "    if output:\n",
        "      if output[-1].endswith('.\\tO'):\n",
        "        output.extend(\"\\n\")\n",
        " \n",
        "  return output\n",
        "\n",
        "  \n",
        "'''\n",
        "for foundation in getCategoryList(\"Nederlandse stichting\"):\n",
        "  for sentence in getSentencesWithSubject(foundation)[0]:\n",
        "    for w in wordPerLine(sentence, getSentencesWithSubject(foundation)[1]):\n",
        "      g.write(w)\n",
        "      g.write(\"\\n\")\n",
        "    g.write(\"\\n\")\n",
        "   \n",
        "g = open('sws.txt')\n",
        "content = g.read()\n",
        "g.close()\n",
        "newContent = content.replace('\\n\\n\\n', '\\n')\n",
        "g = open('sws.txt', 'w')\n",
        "g.write(newContent)\n",
        "\n",
        "\n",
        "for foundation in getCategoryList(\"Nederlandse stichting\"):\n",
        "  for w in wordPerLine(getFirstSection(foundation)[0], getFirstSection(foundation)[1]):\n",
        "    for x in w:\n",
        "      f.write(x)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "f = open('firstsections.txt')\n",
        "content = f.read()\n",
        "f.close()\n",
        "newContent = content.replace('\\n\\n', '\\n')\n",
        "f = open('firstsections.txt', 'w')\n",
        "f.write(newContent)\n",
        "\n",
        "'''\n",
        "g.close()\n",
        "f.close()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Ze6Nx9QQcGy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}